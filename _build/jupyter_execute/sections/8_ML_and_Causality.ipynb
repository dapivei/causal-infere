{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQLNpzhJBsjA"
   },
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dapivei/causal-infere/blob/main/sections/8_ML_and_Causality.ipynb)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsLZI9CzzVrf"
   },
   "source": [
    "# The Interplay between Machine Learning and Causal Inference\n",
    "\n",
    "There is an increasing interest in  \n",
    "\n",
    "1.   how machine learning techniques can improve causal effect estimation\n",
    "2.   how causal inference principles can enhance machine learning algorithms, particularly for robustness\n",
    "\n",
    "The aim of this lab is to give you a broad overview of existing work and research at the intersection of machine learning and causal inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdORTKzfx289"
   },
   "source": [
    "## 1. Machine Learning for Causality\n",
    "\n",
    "The first step in performing causal inference is knowing the causal parameter we are trying to identify and estimate. In this class, we have been mostly focused on the average treatment effect (ATE):\n",
    "\n",
    "\\begin{equation}\n",
    "ATE = \\mathbb{E}[Y(1, U) - Y(0, U)]\n",
    "\\end{equation}\n",
    "\n",
    "We can identify the above equation to form a statistical estimand by conditioning if $S \\perp U$:\n",
    "\n",
    "\\begin{equation}\n",
    "ATE = \\mathbb{E}[Y(1, U)|S=1] - \\mathbb{E}[Y(0, U)|S=0]\n",
    "\\end{equation}\n",
    "\n",
    "Last week, we also learned that the identification of ATE can be facilitated through conditional independence.\n",
    "\n",
    "\\begin{equation}\n",
    "S \\perp U \\ | \\ C\n",
    "\\end{equation}\n",
    "\n",
    "Remember, we cannot just make any (machine-learning) models as being \"causal\" models, but we know the conditions and assumptions under which causality is feasible: descriptive regression $→$ causal regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoUhXIS8hhql"
   },
   "source": [
    "### 1.1 Other Causal Parameters?\n",
    "\n",
    "Actually, you have seen another causal parameter: individualized treatment effect (ITE). It is the most straightforward to write down, but it is also the most difficult one (almost impossible) to identify. Instead of estimating causal effects on a population level, the inidividual level effect is much more helpful as it underpins the promises of things like personalized medicine (one medication could be helpful on average but harmful for a particular individual).\n",
    "\n",
    "\\begin{equation}\n",
    "ITE = Y_i(1, u) - Y_i(0, u)\n",
    "\\end{equation}\n",
    "\n",
    "Instead, an easier quantity to learn is called conditional average treatment effect (CATE). For the rest of the lab, we will use a more standard machine learning notation involving $(Y, X, T)$ and not writing $U$ for simplicity.\n",
    "\n",
    "\\begin{equation}\n",
    "CATE = \\mathbb{E}[Y(1) - Y(0) | X]\n",
    "\\end{equation}\n",
    "\n",
    "If we are simply leveraging the conditional independence assumption for identifying ATE, this can be written as the following:\n",
    "\n",
    "\\begin{equation}\n",
    "ATE = \\mathbb{E}_{X}\\left[\\mathbb{E}[Y(1) - Y(0) | X]\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-FNDUA42Too"
   },
   "source": [
    "### 1.2 Statistical Estimands\n",
    "\n",
    "Given a dataset of $D = \\{Y, X, T\\}$, we can identify the above ATE and CATE with two major assumptions:\n",
    "\n",
    "\n",
    "1. Ignorability: $Y(1), Y(0) \\perp T | X$\n",
    "2. Positivity: $p(T=t | X = \\mathbf{x}) >0, ∀t, \\mathbf{x}$\n",
    "\n",
    "We have seen the first one! It reads as the treatment assignment does not influence the potential outcomes condition on the features $X$, i.e., the treatment group and the control group should have similar characteristics ($S \\perp U | C$).\n",
    "\n",
    "We haven't talked much about positivity yet as we are just beginning to touch on control variables, but this is intuitive: there always should be people taking all the treatment options among the subpopulation on which they are conditioned. For example, if all college graduates do not attend the job training program, this assumption will be violated.\n",
    "\n",
    "We can derive the statistical estimand for CATE and ATE just as in lectures.\n",
    "\n",
    "\\begin{align}\n",
    "CATE &= \\mathbb{E}[Y(1) - Y(0) | X] \\\\\n",
    "&= \\mathbb{E}[Y(1)|X] - \\mathbb{E}[Y(0) | X] \\\\\n",
    "&= \\mathbb{E}[Y(1)|T=1, X] - \\mathbb{E}[Y(0) |T=0, X] \\\\\n",
    "&= \\mathbb{E}[Y|T=1, X] - \\mathbb{E}[Y|T=0, X] \\\\\n",
    "\\end{align}\n",
    "\n",
    "and,\n",
    "\n",
    "\\begin{align}\n",
    "ATE &= \\mathbb{E}_{X}\\left[\\mathbb{E}[Y(1) - Y(0) | X]\\right] \\\\\n",
    "&= \\mathbb{E}_{X}\\left[\\mathbb{E}[Y|T=1, X] - \\mathbb{E}[Y|T=0, X]\\right] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa2tK4Xz4BcR"
   },
   "source": [
    "### 1.3 Advanced Estimation\n",
    "\n",
    "When the assumptions hold, we can simply regress the outcome on the treatment and features, and we have shown in class that the coefficient for the treatment variable is the corresponding ATE. For CATE, we can add an interaction term between treatment and the conditioned feature like $\\beta XT$.\n",
    "\n",
    "However, what if the underlying true model is **not** a linear model?\n",
    "\n",
    "In general, we can use any model class for estimating $\\mathbb{E}[Y|T=1, X]$ and $\\mathbb{E}[Y|T=0, X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jTd1RmXIgt8"
   },
   "source": [
    "#### 1.3.1 S-learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vZqaoAaHqpK"
   },
   "source": [
    "We can use a single model $f(\\mathbf{x}_i, t_i)$ to predict $Y$ based on $X$ and $T$.\n",
    "\n",
    "![S-learner](https://github.com/dapivei/causal-infere/blob/main/images/S-learner.png?raw=true)\n",
    "\n",
    "image credit: [link](https://csc2541-2022.github.io/lectures/CSC2541_lecture4_estimation.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vkalx8NHrSB"
   },
   "source": [
    "Let's simulate some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-sDFhIYqGYF7",
    "outputId": "2dc075e7-8c7d-4000-83f1-7eecb48f8cd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>T</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>1.523030</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>1</td>\n",
       "      <td>1.145004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.234137</td>\n",
       "      <td>1.579213</td>\n",
       "      <td>0.767435</td>\n",
       "      <td>-0.469474</td>\n",
       "      <td>0.542560</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.081058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.463418</td>\n",
       "      <td>-0.465730</td>\n",
       "      <td>0.241962</td>\n",
       "      <td>-1.913280</td>\n",
       "      <td>-1.724918</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.284728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.562288</td>\n",
       "      <td>-1.012831</td>\n",
       "      <td>0.314247</td>\n",
       "      <td>-0.908024</td>\n",
       "      <td>-1.412304</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.214069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.465649</td>\n",
       "      <td>-0.225776</td>\n",
       "      <td>0.067528</td>\n",
       "      <td>-1.424748</td>\n",
       "      <td>-0.544383</td>\n",
       "      <td>0</td>\n",
       "      <td>1.651123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5  T         Y\n",
       "0  0.496714 -0.138264  0.647689  1.523030 -0.234153  1  1.145004\n",
       "1 -0.234137  1.579213  0.767435 -0.469474  0.542560  1 -0.081058\n",
       "2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918  0 -1.284728\n",
       "3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304  1 -1.214069\n",
       "4  1.465649 -0.225776  0.067528 -1.424748 -0.544383  0  1.651123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# simulate features\n",
    "np.random.seed(42)\n",
    "n = 5000\n",
    "X = np.random.normal(0, 1, (n, 5))\n",
    "\n",
    "# simulate treatment assignment (binary treatment)\n",
    "T = np.random.binomial(1, 0.5, n)\n",
    "\n",
    "# define true treatment effect function\n",
    "def true_effect(x):\n",
    "    return 2 * x[:, 0]\n",
    "\n",
    "# simulate outcomes\n",
    "Y = true_effect(X) * T + X[:, 1] + np.random.normal(0, 1, n)\n",
    "\n",
    "# create a DataFrame\n",
    "data = pd.DataFrame(X, columns=[f'X{i}' for i in range(1, 6)])\n",
    "data['T'] = T\n",
    "data['Y'] = Y\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CiryhGYD6uBH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# features and target\n",
    "X_all = data.drop('Y', axis=1)\n",
    "y_all = data['Y']\n",
    "\n",
    "# perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# separate T and X in training and test sets\n",
    "T_train = X_train['T']\n",
    "X_train_features = X_train.drop('T', axis=1)\n",
    "\n",
    "T_test = X_test['T']\n",
    "X_test_features = X_test.drop('T', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuNgNWTzIhNT",
    "outputId": "80505e00-e637-4d86-f3ed-f10215852951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using S-Learner: -0.0192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# include T in the features\n",
    "X_train_s = X_train.copy()\n",
    "y_train_s = y_train.copy()\n",
    "\n",
    "# train the S-Learner\n",
    "model_s = RandomForestRegressor(random_state=42)\n",
    "model_s.fit(X_train_s, y_train_s)\n",
    "\n",
    "# prepare test data with T=1 and T=0\n",
    "X_test_T1 = X_test.copy()\n",
    "X_test_T1['T'] = 1\n",
    "\n",
    "X_test_T0 = X_test.copy()\n",
    "X_test_T0['T'] = 0\n",
    "\n",
    "# predict outcomes\n",
    "y_pred_T1 = model_s.predict(X_test_T1)\n",
    "y_pred_T0 = model_s.predict(X_test_T0)\n",
    "\n",
    "# estimate ITE\n",
    "ite_s = y_pred_T1 - y_pred_T0\n",
    "\n",
    "# compute ATE\n",
    "ate_s = np.mean(ite_s)\n",
    "\n",
    "print(f\"Estimated ATE using S-Learner: {ate_s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87MFEtEi24aH"
   },
   "source": [
    "What are some potential problems if we have high-dimensional $X$, i.e., $D$ is very large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v4BxFgmJHkb"
   },
   "source": [
    "#### 1.3.2 T-learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeQhgiBH3RbV"
   },
   "source": [
    "Alternatively, we can build separate models $f_1(\\mathbf{x}_i, T=1)$ and $f_0(\\mathbf{x}_i, T=0)$ for the treatment and control group.\n",
    "\n",
    "![T-learner](https://github.com/dapivei/causal-infere/blob/main/images/T-learner.png?raw=true)\n",
    "\n",
    "image credit: [link](https://csc2541-2022.github.io/lectures/CSC2541_lecture4_estimation.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1JU0pIx4ALS",
    "outputId": "785502dd-c064-4c51-babb-5fb3f5052275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using T-Learner: -0.0338\n"
     ]
    }
   ],
   "source": [
    "# split training data by treatment\n",
    "train_data = X_train.copy()\n",
    "train_data['Y'] = y_train\n",
    "\n",
    "train_treated = train_data[train_data['T'] == 1]\n",
    "train_control = train_data[train_data['T'] == 0]\n",
    "\n",
    "# features and target for treated group\n",
    "X_treated = train_treated.drop(['Y', 'T'], axis=1)\n",
    "y_treated = train_treated['Y']\n",
    "\n",
    "# features and target for control group\n",
    "X_control = train_control.drop(['Y', 'T'], axis=1)\n",
    "y_control = train_control['Y']\n",
    "\n",
    "# train models\n",
    "model_treated = RandomForestRegressor(random_state=42)\n",
    "model_control = RandomForestRegressor(random_state=42)\n",
    "\n",
    "model_treated.fit(X_treated, y_treated)\n",
    "model_control.fit(X_control, y_control)\n",
    "\n",
    "# predict outcomes on test set\n",
    "y_pred_treated = model_treated.predict(X_test_features)\n",
    "y_pred_control = model_control.predict(X_test_features)\n",
    "\n",
    "# estimate ITE\n",
    "ite_t = y_pred_treated - y_pred_control\n",
    "\n",
    "# compute ATE\n",
    "ate_t = np.mean(ite_t)\n",
    "\n",
    "print(f\"Estimated ATE using T-Learner: {ate_t:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6ScPxzv-Psf"
   },
   "source": [
    "#### 1.3.3 TAR-Net\n",
    "\n",
    "What are some of the drawbacks of the above approaches?\n",
    "\n",
    "![Tradeoff](https://github.com/dapivei/causal-infere/blob/main/images/Trade-off.png?raw=true)\n",
    "\n",
    "[Johansson, Shalit, and Sontag, 2016](https://arxiv.org/abs/1606.03976) leveraged neural networks to overcome the above challenge via learning a common representation.\n",
    "\n",
    "(Additional regularization needed to ensure balanced representations for both groups)\n",
    "\n",
    "![TAR-Net](https://github.com/dapivei/causal-infere/blob/main/images/TAR-Net.png?raw=true)\n",
    "\n",
    "image credit: [link](https://csc2541-2022.github.io/lectures/CSC2541_lecture4_estimation.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UHxQ4BPJ-ZLq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# prepare training data tensors\n",
    "X_train_tensor = torch.tensor(X_train_features.values, dtype=torch.float32)\n",
    "T_train_tensor = torch.tensor(T_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "Y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, T_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# define TARNet model\n",
    "class TARNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TARNet, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.outcome_0 = nn.Sequential(\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "        self.outcome_1 = nn.Sequential(\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        phi = self.shared(x)\n",
    "        y0 = self.outcome_0(phi)\n",
    "        y1 = self.outcome_1(phi)\n",
    "        y_pred = t * y1 + (1 - t) * y0\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, x):\n",
    "        phi = self.shared(x)\n",
    "        y0 = self.outcome_0(phi)\n",
    "        y1 = self.outcome_1(phi)\n",
    "        return y0, y1\n",
    "\n",
    "# initialize model, loss function, and optimizer\n",
    "model = TARNet(input_dim=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SjRrzL_AL0f",
    "outputId": "89c34a40-9b59-40aa-8a7c-5e4d01a9fdfa"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, Y_batch)\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 10\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 432\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, T_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch, T_batch)\n",
    "        loss = criterion(y_pred, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UOm7wgWAPQ6",
    "outputId": "87bd5b5e-265d-4129-c775-c95eae9cf8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using TARNet: -0.0584\n"
     ]
    }
   ],
   "source": [
    "# prepare test data tensors\n",
    "X_test_tensor = torch.tensor(X_test_features.values, dtype=torch.float32)\n",
    "\n",
    "# estimate ITE\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0_pred, y1_pred = model.predict(X_test_tensor)\n",
    "    ite_tar = (y1_pred - y0_pred).numpy().flatten()\n",
    "\n",
    "# compute ATE\n",
    "ate_tar = np.mean(ite_tar)\n",
    "\n",
    "print(f\"Estimated ATE using TARNet: {ate_tar:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn4NBWTCJP4B",
    "outputId": "8f80453e-a548-4e81-d85d-1dd97c641c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE:    -0.0276\n"
     ]
    }
   ],
   "source": [
    "# Compute true ATE\n",
    "X_test_array = X_test_features.values\n",
    "true_ite = true_effect(X_test_array)\n",
    "true_ate = np.mean(true_ite)\n",
    "print(f\"True ATE:    {true_ate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_UkNVoB8JJU"
   },
   "source": [
    "### 1.3.4 Double Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqQe5p5qCCkk"
   },
   "source": [
    "TAR-Net still suffers a few problems:\n",
    "\n",
    "\n",
    "*   does not support other machine learning models\n",
    "*   only applies to binary treatment\n",
    "*   no uncertainty quantification\n",
    "\n",
    "Double ML provides a framework that leverage general machine learning models to obtain **unbiased** causal effect estimates with *fast convergence* and *confidence intervals*, and it is still among the best methods in causal effect estimation. (You just need to know such a method exists. Further reading: [link](https://econml.azurewebsites.net/spec/estimation/dml.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtUTVEY86Xj"
   },
   "source": [
    "## 2. Causal Inference for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmXWMh15KXWh"
   },
   "source": [
    "We will not go into technical detail in this section but aim to provide you with a high-level intuition on how causality can help predictions.\n",
    "\n",
    "The standard machine-learning pipeline assumes that the test examples are I.I.D as the training data distribution. However, in practice, this is often violated. For instance, we train an imaging model to read CT scans using data from NYU Langone, will this model do well if it is deployed in other countries w. different imaging equipment and lighting conditions?\n",
    "\n",
    "![OOD-eg](https://github.com/dapivei/causal-infere/blob/main/images/OOD.png?raw=true)\n",
    "\n",
    "image credit: [link](https://arxiv.org/abs/1911.08731)\n",
    "\n",
    "We hope to build models that are robust to changes to test distribution, i.e., achieving out-of-distribution generalization. Why is this difficult?\n",
    "\n",
    "During training, the model can easily pick up on predictive signals from features that are relevant in distribution, e.g., image background. This relationship will not hold in other data distributions.\n",
    "\n",
    "However, causal relationships are reliable associations across different distributions. We can formalize it through causality so that we can build models that only rely on causal features/representations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}