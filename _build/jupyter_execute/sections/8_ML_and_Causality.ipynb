{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQLNpzhJBsjA"
   },
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dapivei/causal-infere/blob/main/sections/8_ML_and_Causality.ipynb)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsLZI9CzzVrf"
   },
   "source": [
    "# The Interplay between Machine Learning and Causal Inference\n",
    "\n",
    "There is an increasing interest in  \n",
    "\n",
    "1.   how machine learning techniques can improve causal effect estimation\n",
    "2.   how causal inference principles can enhance machine learning algorithms, particularly for robustness\n",
    "\n",
    "The aim of this lab is to give you a broad overview of existing work and research at the intersection of machine learning and causal inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdORTKzfx289"
   },
   "source": [
    "## Machine Learning for Causality\n",
    "\n",
    "The first step in performing causal inference is knowing the causal parameter we are trying to identify and estimate. In this class, we have been mostly focused on the average treatment effect (ATE):\n",
    "\n",
    "\\begin{equation}\n",
    "ATE = \\mathbb{E}[Y(1, U) - Y(0, U)]\n",
    "\\end{equation}\n",
    "\n",
    "We can identify the above equation to form a statistical estimand by conditioning if $S \\perp U$:\n",
    "\n",
    "\\begin{equation}\n",
    "ATE = \\mathbb{E}[Y(1, U)|S=1] - \\mathbb{E}[Y(0, U)|S=0]\n",
    "\\end{equation}\n",
    "\n",
    "Last week, we also learned that the identification of ATE can be facilitated through conditional independence.\n",
    "\n",
    "\\begin{equation}\n",
    "S \\perp U \\ | \\ C\n",
    "\\end{equation}\n",
    "\n",
    "Remember, we cannot just make any (machine-learning) models as being \"causal\" models, but we know the conditions and assumptions under which causality is feasible: descriptive regression $→$ causal regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoUhXIS8hhql"
   },
   "source": [
    "### Other Causal Parameters?\n",
    "\n",
    "Actually, you have seen another causal parameter: individualized treatment effect (ITE). It is the most straightforward to write down, but it is also the most difficult one (almost impossible) to identify. Instead of estimating causal effects on a population level, the inidividual level effect is much more helpful as it underpins the promises of things like personalized medicine (one medication could be helpful on average but harmful for a particular individual).\n",
    "\n",
    "\\begin{equation}\n",
    "ITE = Y_i(1, u) - Y_i(0, u)\n",
    "\\end{equation}\n",
    "\n",
    "Instead, an easier quantity to learn is called conditional average treatment effect (CATE). For the rest of the lab, we will use a more standard machine learning notation involving $(Y, X, T)$ and not writing $U$ for simplicity.\n",
    "\n",
    "\\begin{equation}\n",
    "CATE = \\mathbb{E}[Y(1) - Y(0) | X]\n",
    "\\end{equation}\n",
    "\n",
    "If we are simply leveraging the conditional independence assumption for identifying ATE, this can be written as the following:\n",
    "\n",
    "\\begin{equation}\n",
    "ATE = \\mathbb{E}_{X}\\left[\\mathbb{E}[Y(1) - Y(0) | X]\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-FNDUA42Too"
   },
   "source": [
    "### Statistical Estimands\n",
    "\n",
    "Given a dataset of $D = \\{Y, X, T\\}$, we can identify the above ATE and CATE with two major assumptions:\n",
    "\n",
    "\n",
    "1. Ignorability: $Y(1), Y(0) \\perp T | X$\n",
    "2. Positivity: $p(T=t | X = \\mathbf{x}) >0, ∀t, \\mathbf{x}$\n",
    "\n",
    "We have seen the first one! It reads as the treatment assignment does not influence the potential outcomes condition on the features $X$, i.e., the treatment group and the control group should have similar characteristics ($S \\perp U | C$).\n",
    "\n",
    "We haven't talked much about positivity yet as we are just beginning to touch on control variables, but this is intuitive: there always should be people taking all the treatment options among the subpopulation on which they are conditioned. For example, if all college graduates do not attend the job training program, this assumption will be violated.\n",
    "\n",
    "We can derive the statistical estimand for CATE and ATE just as in lectures.\n",
    "\n",
    "\\begin{align}\n",
    "CATE &= \\mathbb{E}[Y(1) - Y(0) | X] \\\\\n",
    "&= \\mathbb{E}[Y(1)|X] - \\mathbb{E}[Y(0) | X] \\\\\n",
    "&= \\mathbb{E}[Y(1)|T=1, X] - \\mathbb{E}[Y(0) |T=0, X] \\\\\n",
    "&= \\mathbb{E}[Y|T=1, X] - \\mathbb{E}[Y|T=0, X] \\\\\n",
    "\\end{align}\n",
    "\n",
    "and,\n",
    "\n",
    "\\begin{align}\n",
    "ATE &= \\mathbb{E}_{X}\\left[\\mathbb{E}[Y(1) - Y(0) | X]\\right] \\\\\n",
    "&= \\mathbb{E}_{X}\\left[\\mathbb{E}[Y|T=1, X] - \\mathbb{E}[Y|T=0, X]\\right] \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa2tK4Xz4BcR"
   },
   "source": [
    "### Advanced Estimation\n",
    "\n",
    "When the assumptions hold, we can simply regress the outcome on the treatment and features, and we have shown in class that the coefficient for the treatment variable is the corresponding ATE. For CATE, we can add an interaction term between treatment and the conditioned feature like $\\beta XT$.\n",
    "\n",
    "However, what if the underlying true model is **not** a linear model?\n",
    "\n",
    "In general, we can use any model class for estimating $\\mathbb{E}[Y|T=1, X]$ and $\\mathbb{E}[Y|T=0, X]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jTd1RmXIgt8"
   },
   "source": [
    "#### S-learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vZqaoAaHqpK"
   },
   "source": [
    "We can use a single model $f(\\mathbf{x}_i, t_i)$ to predict $Y$ based on $X$ and $T$.\n",
    "\n",
    "![S-learner](https://github.com/dapivei/causal-infere/blob/main/images/S-learner.png?raw=true)\n",
    "\n",
    "image credit: [link](https://csc2541-2022.github.io/lectures/CSC2541_lecture4_estimation.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vkalx8NHrSB"
   },
   "source": [
    "Let's simulate some data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-sDFhIYqGYF7",
    "outputId": "2dc075e7-8c7d-4000-83f1-7eecb48f8cd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>T</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496714</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>1.523030</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>1</td>\n",
       "      <td>1.145004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.234137</td>\n",
       "      <td>1.579213</td>\n",
       "      <td>0.767435</td>\n",
       "      <td>-0.469474</td>\n",
       "      <td>0.542560</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.081058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.463418</td>\n",
       "      <td>-0.465730</td>\n",
       "      <td>0.241962</td>\n",
       "      <td>-1.913280</td>\n",
       "      <td>-1.724918</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.284728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.562288</td>\n",
       "      <td>-1.012831</td>\n",
       "      <td>0.314247</td>\n",
       "      <td>-0.908024</td>\n",
       "      <td>-1.412304</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.214069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.465649</td>\n",
       "      <td>-0.225776</td>\n",
       "      <td>0.067528</td>\n",
       "      <td>-1.424748</td>\n",
       "      <td>-0.544383</td>\n",
       "      <td>0</td>\n",
       "      <td>1.651123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5  T         Y\n",
       "0  0.496714 -0.138264  0.647689  1.523030 -0.234153  1  1.145004\n",
       "1 -0.234137  1.579213  0.767435 -0.469474  0.542560  1 -0.081058\n",
       "2 -0.463418 -0.465730  0.241962 -1.913280 -1.724918  0 -1.284728\n",
       "3 -0.562288 -1.012831  0.314247 -0.908024 -1.412304  1 -1.214069\n",
       "4  1.465649 -0.225776  0.067528 -1.424748 -0.544383  0  1.651123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# simulate features\n",
    "np.random.seed(42)\n",
    "n = 5000\n",
    "X = np.random.normal(0, 1, (n, 5))\n",
    "\n",
    "# simulate treatment assignment (binary treatment)\n",
    "T = np.random.binomial(1, 0.5, n)\n",
    "\n",
    "# define true treatment effect function\n",
    "def true_effect(x):\n",
    "    return 2 * x[:, 0]\n",
    "\n",
    "# simulate outcomes\n",
    "Y = true_effect(X) * T + X[:, 1] + np.random.normal(0, 1, n)\n",
    "\n",
    "# create a DataFrame\n",
    "data = pd.DataFrame(X, columns=[f'X{i}' for i in range(1, 6)])\n",
    "data['T'] = T\n",
    "data['Y'] = Y\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CiryhGYD6uBH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# features and target\n",
    "X_all = data.drop('Y', axis=1)\n",
    "y_all = data['Y']\n",
    "\n",
    "# perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# separate T and X in training and test sets\n",
    "T_train = X_train['T']\n",
    "X_train_features = X_train.drop('T', axis=1)\n",
    "\n",
    "T_test = X_test['T']\n",
    "X_test_features = X_test.drop('T', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuNgNWTzIhNT",
    "outputId": "80505e00-e637-4d86-f3ed-f10215852951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using S-Learner: -0.0192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# include T in the features\n",
    "X_train_s = X_train.copy()\n",
    "y_train_s = y_train.copy()\n",
    "\n",
    "# train the S-Learner\n",
    "model_s = RandomForestRegressor(random_state=42)\n",
    "model_s.fit(X_train_s, y_train_s)\n",
    "\n",
    "# prepare test data with T=1 and T=0\n",
    "X_test_T1 = X_test.copy()\n",
    "X_test_T1['T'] = 1\n",
    "\n",
    "X_test_T0 = X_test.copy()\n",
    "X_test_T0['T'] = 0\n",
    "\n",
    "# predict outcomes\n",
    "y_pred_T1 = model_s.predict(X_test_T1)\n",
    "y_pred_T0 = model_s.predict(X_test_T0)\n",
    "\n",
    "# estimate ITE\n",
    "ite_s = y_pred_T1 - y_pred_T0\n",
    "\n",
    "# compute ATE\n",
    "ate_s = np.mean(ite_s)\n",
    "\n",
    "print(f\"Estimated ATE using S-Learner: {ate_s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87MFEtEi24aH"
   },
   "source": [
    "What are some potential problems if we have high-dimensional $X$, i.e., $D$ is very large?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v4BxFgmJHkb"
   },
   "source": [
    "#### T-learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeQhgiBH3RbV"
   },
   "source": [
    "Alternatively, we can build separate models $f_1(\\mathbf{x}_i, T=1)$ and $f_0(\\mathbf{x}_i, T=0)$ for the treatment and control group.\n",
    "\n",
    "![T-learner](https://github.com/dapivei/causal-infere/blob/main/images/T-learner.png?raw=true)\n",
    "\n",
    "image credit: [link](https://csc2541-2022.github.io/lectures/CSC2541_lecture4_estimation.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1JU0pIx4ALS",
    "outputId": "785502dd-c064-4c51-babb-5fb3f5052275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using T-Learner: -0.0338\n"
     ]
    }
   ],
   "source": [
    "# split training data by treatment\n",
    "train_data = X_train.copy()\n",
    "train_data['Y'] = y_train\n",
    "\n",
    "train_treated = train_data[train_data['T'] == 1]\n",
    "train_control = train_data[train_data['T'] == 0]\n",
    "\n",
    "# features and target for treated group\n",
    "X_treated = train_treated.drop(['Y', 'T'], axis=1)\n",
    "y_treated = train_treated['Y']\n",
    "\n",
    "# features and target for control group\n",
    "X_control = train_control.drop(['Y', 'T'], axis=1)\n",
    "y_control = train_control['Y']\n",
    "\n",
    "# train models\n",
    "model_treated = RandomForestRegressor(random_state=42)\n",
    "model_control = RandomForestRegressor(random_state=42)\n",
    "\n",
    "model_treated.fit(X_treated, y_treated)\n",
    "model_control.fit(X_control, y_control)\n",
    "\n",
    "# predict outcomes on test set\n",
    "y_pred_treated = model_treated.predict(X_test_features)\n",
    "y_pred_control = model_control.predict(X_test_features)\n",
    "\n",
    "# estimate ITE\n",
    "ite_t = y_pred_treated - y_pred_control\n",
    "\n",
    "# compute ATE\n",
    "ate_t = np.mean(ite_t)\n",
    "\n",
    "print(f\"Estimated ATE using T-Learner: {ate_t:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6ScPxzv-Psf"
   },
   "source": [
    "#### TAR-Net\n",
    "\n",
    "What are some of the drawbacks of the above approaches?\n",
    "\n",
    "![Tradeoff](https://github.com/dapivei/causal-infere/blob/main/images/Trade-off.png?raw=true)\n",
    "\n",
    "[Johansson, Shalit, and Sontag, 2016](https://arxiv.org/abs/1606.03976) leveraged neural networks to overcome the above challenge via learning a common representation.\n",
    "\n",
    "(Additional regularization needed to ensure balanced representations for both groups)\n",
    "\n",
    "![TAR-Net](https://github.com/dapivei/causal-infere/blob/main/images/TAR-Net.png?raw=true)\n",
    "\n",
    "image credit: [link](https://csc2541-2022.github.io/lectures/CSC2541_lecture4_estimation.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UHxQ4BPJ-ZLq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# prepare training data tensors\n",
    "X_train_tensor = torch.tensor(X_train_features.values, dtype=torch.float32)\n",
    "T_train_tensor = torch.tensor(T_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "Y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, T_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# define TARNet model\n",
    "class TARNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TARNet, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.outcome_0 = nn.Sequential(\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "        self.outcome_1 = nn.Sequential(\n",
    "            nn.Linear(100, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        phi = self.shared(x)\n",
    "        y0 = self.outcome_0(phi)\n",
    "        y1 = self.outcome_1(phi)\n",
    "        y_pred = t * y1 + (1 - t) * y0\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, x):\n",
    "        phi = self.shared(x)\n",
    "        y0 = self.outcome_0(phi)\n",
    "        y1 = self.outcome_1(phi)\n",
    "        return y0, y1\n",
    "\n",
    "# initialize model, loss function, and optimizer\n",
    "model = TARNet(input_dim=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SjRrzL_AL0f",
    "outputId": "89c34a40-9b59-40aa-8a7c-5e4d01a9fdfa"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, T_batch, Y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, Y_batch)\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m, in \u001b[0;36mTARNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 39\u001b[0m     phi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     y0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutcome_0(phi)\n\u001b[1;32m     41\u001b[0m     y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutcome_1(phi)\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, T_batch, Y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch, T_batch)\n",
    "        loss = criterion(y_pred, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UOm7wgWAPQ6",
    "outputId": "87bd5b5e-265d-4129-c775-c95eae9cf8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using TARNet: -0.0584\n"
     ]
    }
   ],
   "source": [
    "# prepare test data tensors\n",
    "X_test_tensor = torch.tensor(X_test_features.values, dtype=torch.float32)\n",
    "\n",
    "# estimate ITE\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0_pred, y1_pred = model.predict(X_test_tensor)\n",
    "    ite_tar = (y1_pred - y0_pred).numpy().flatten()\n",
    "\n",
    "# compute ATE\n",
    "ate_tar = np.mean(ite_tar)\n",
    "\n",
    "print(f\"Estimated ATE using TARNet: {ate_tar:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rn4NBWTCJP4B",
    "outputId": "8f80453e-a548-4e81-d85d-1dd97c641c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE:    -0.0276\n"
     ]
    }
   ],
   "source": [
    "# Compute true ATE\n",
    "X_test_array = X_test_features.values\n",
    "true_ite = true_effect(X_test_array)\n",
    "true_ate = np.mean(true_ite)\n",
    "print(f\"True ATE:    {true_ate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_UkNVoB8JJU"
   },
   "source": [
    "### Double Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqQe5p5qCCkk"
   },
   "source": [
    "TAR-Net still suffers a few problems:\n",
    "\n",
    "\n",
    "*   does not support other machine learning models\n",
    "*   only applies to binary treatment\n",
    "*   no uncertainty quantification\n",
    "\n",
    "Double ML provides a framework that leverage general machine learning models to obtain **unbiased** causal effect estimates with *fast convergence* and *confidence intervals*, and it is still among the best methods in causal effect estimation. (You just need to know such a method exists. Further reading: [link](https://econml.azurewebsites.net/spec/estimation/dml.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtUTVEY86Xj"
   },
   "source": [
    "## Causal Inference for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmXWMh15KXWh"
   },
   "source": [
    "We will not go into technical detail in this section but aim to provide you with a high-level intuition on how causality can help predictions.\n",
    "\n",
    "The standard machine-learning pipeline assumes that the test examples are I.I.D as the training data distribution. However, in practice, this is often violated. For instance, we train an imaging model to read CT scans using data from NYU Langone, will this model do well if it is deployed in other countries w. different imaging equipment and lighting conditions?\n",
    "\n",
    "![OOD-eg](https://github.com/dapivei/causal-infere/blob/main/images/OOD.png?raw=true)\n",
    "\n",
    "image credit: [link](https://arxiv.org/abs/1911.08731)\n",
    "\n",
    "We hope to build models that are robust to changes to test distribution, i.e., achieving out-of-distribution generalization. Why is this difficult?\n",
    "\n",
    "During training, the model can easily pick up on predictive signals from features that are relevant in distribution, e.g., image background. This relationship will not hold in other data distributions.\n",
    "\n",
    "However, causal relationships are reliable associations across different distributions. We can formalize it through causality so that we can build models that only rely on causal features/representations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}